{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloqade High-Performance Capabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John Long, *Scientific Software Developer*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook attempts to display some of the High-Performance capabilities that Bloqade has built-in. While prior examples may be designed to run in their own notebooks the nature of some of the capabilities presented (requiring the restarting or potential modification of kernels via their startup arguments) means this particular notebook is **NOT DESIGNED TO BE RUN IN ITS ENTIRETY**. If you would like to follow along with certain examples you will need to duplicate them outside the notebook in a separate Julia session. \n",
    "\n",
    "First though, it's worth taking a look at how Bloqade simulates things from a birds-eye view to understand the benefits of each High-Performance method and why the exist in the first place:\n",
    "\n",
    "1. You define a Hamiltonian expression of interest (usually with `rydberg_h`)\n",
    "2. The Hamiltonian expression, along with a register (you can think of this as the statevector), and amount of time to simulate evolution for, are passed into a `SchrodingerProblem`\n",
    "3. The `SchrodingerProblem` \"lowers\" or reduces the expression into a series of different Sparse Matrix formats that represent certain terms in the Hamiltonian\n",
    "4. The register that was passed in earlier is repeatedly multiplied against these matrices via Sparse-Matrix Dense-Vector (SpMV) multiplication as part of the Runge-Kutta solver."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we optimize any part of these steps? Yes! The optimizations essentially fall into two categories:\n",
    "\n",
    "* Memory Efficiency - Store a smaller amount of data while sacrificing as little accuracy as possible\n",
    "* Execution Efficiency - Keeping the format of the problem the same but speeding up the actual execution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Efficiency - Blockade Subspace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simulate larger system sizes by realizing that there are scenarios where it isn't necessary to preserve all $2^n$ possible states. If two atoms blockade each other by being close enough and attempting to simultaneously be in the Rydberg state ($\\langle|rr\\rangle$), the probability of getting a double Rydberg Excitation state is impossible. Thus we can safely get rid of these states but we should still exercise care because of the possibility of dropping non-trivial contributions to the dynamics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual code and a more detailed explanation can be found in this accompanying notebook: [Blockade Subspace](Section_V_Blockade_Subspace.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Efficiency - Multithreading and GPU Support"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster simulation times in Bloqade are achievable through multi-core CPUs via multithreading support and on Nvidia GPUs via CUDA support.\n",
    "\n",
    "This is all thanks to Julia's support for *multiple dispatch*, which allows you to define the same function but for different types. If the register is changed to a GPU-specific type, then all arithmetic operations involving the register call their GPU variants. \n",
    "\n",
    "In a similar vein, for multithreading the sparse matrices will now invoke a parallelized matrix-vector multiplication function instead of the default single-threaded version simply through some automatic type wrapping that occurs behind the scenes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multithreading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before showing the speedup attainable via multi-threading let's see a simulation run without it. In the cell below has code to measure the run time for the simulation of an adiabatic sweep on a 4 x 4 lattice. This leads to two possible final states in equal superposition, both with an alternating Rydberg-Ground pattern (although the orders are flipped):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE:* Owing to Julia's method of precompilation, the first time you run any new function there will be some overhead. The function should be run a second time to give the most accurate benchmarking results. If you are looking avoid this precompilation when running a computationally intensive simulation on the first run, you can run a significantly smaller problem (say, one atom) with the same function calls and this should get the precompilation out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Bloqade\n",
    "\n",
    "# can show speedup with different backends\n",
    "function lattice_sweep_benchmark()\n",
    "\n",
    "    # 4x4 lattice of atoms with adiabatic sweep to create something *like* Z2 ordered phase,\n",
    "    # although degeneracy present due to even number of sites\n",
    "    atom_distance = 6.7;\n",
    "    atoms = generate_sites(SquareLattice(), 4, 4, scale = atom_distance);\n",
    "\n",
    "    # define waveforms\n",
    "    Ω_max = 2π * 2.5;\n",
    "    Δ_val = 2π * 10.0;\n",
    "    total_time = 3.0;\n",
    "    time_ramp  = total_time * 0.083;\n",
    "    clocks = [0, time_ramp, total_time - time_ramp, total_time];\n",
    "\n",
    "    Ω = piecewise_linear(clocks = clocks, values = [0.0, Ω_max, Ω_max, 0.0]); \n",
    "    Δ = piecewise_linear(clocks = clocks, values = [-Δ_val, -Δ_val, Δ_val, Δ_val]);\n",
    "    ϕ = constant(;duration = total_time, value = π);\n",
    "\n",
    "    h = rydberg_h(atoms; Ω=Ω, Δ=Δ, ϕ=ϕ);\n",
    "\n",
    "    reg = zero_state(16);\n",
    "\n",
    "    total_time = 3.0\n",
    "    println(\"Creating SchrodingerProblem...\")\n",
    "    prob = SchrodingerProblem(reg, total_time, h;algo=DP8());\n",
    "    println(\"Running Emulation...\")\n",
    "    # print the amount of time it took to run this\n",
    "    # Due to Julia's precompilation, the first time this runs will incur a precompilation overhead. \n",
    "    # Running a second time will give a more accurate result.\n",
    "\n",
    "    # Benchmarking with @time is generally discouraged in Julia but for our purposes is useful enough to show \n",
    "    # the speedup of multithreading\n",
    "    @time emulate!(prob);\n",
    "\n",
    "    return nothing\n",
    "\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's enable multithreading. Multithreading is supported via the `BloqadeExpr` subpackage which supports two different *backends*, each designed for slightly different simulation scenarios.\n",
    "\n",
    "* `ThreadedSparseCSR` - Minimal/no overhead (other than thread spawning) and is good for balanced matrices (number of non-zeros per row remains consistent)\n",
    "* `ParallelMergeCSR` - Pay an up front cost but ensure optimal balancing of matrix-vector multiplication workload across multiple cores (avoid problem of having one core essentially stop other cores from doing useful work), good for unbalanced matrices (possible when employing the blockade subspace optimization OR you end up defining a phase component.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to enable multithreading is the following:\n",
    "\n",
    "1. Launch Julia with the desired number of threads to use through `julia -t num_of_threads`\n",
    "2. Create a Julia environment (this can be done by creating an empty folder for your project and upon navigating to it, going into the package management mode of a Julia session and typing `activate .` . The creation of the new environment should be successful if you see the text before `pkg>` turn into the name of the folder itself (e.g. `(my-empty-folder)>`))\n",
    "3. Add `Bloqade` and `BloqadeExpr` to the environment (while you still see `pkg>` just type `add Bloqade` and `add BloqadeExpr` afterwards)\n",
    "4. Add a file called `LocalPreferences.toml` to the same folder you were working in (your directory structure should look something like the following below):\n",
    "\n",
    "```\n",
    ".\n",
    "├── Mainfest.toml\n",
    "├── Project.toml\n",
    "└── LocalPreferences.toml\n",
    "\n",
    "```\n",
    "\n",
    "`LocalPreferences.toml` should have the following as its contents:\n",
    "\n",
    "```\n",
    "[BloqadeExpr]\n",
    "backend = \"ParallelMergeCSR\"\n",
    "```\n",
    "Where backend can be set to `\"ParallelMergeCSR\"`, `\"ThreadedSparseCSR\"` or `\"BloqadeExpr\"` (`BloqadeExpr` is the default, single-threaded backend. Even if you launch Julia with multiple threads, Bloqade's default behavior is single-threaded and you must explicitly specify that you want multithreading by supplying a different backend).\n",
    "\n",
    "5. Ensure that Bloqade has the right multithreading backend via `julia> using BloqadeExpr` and then `julia> BloqadeExpr.backend`. You should see the same string you set in the `LocalPreferences.toml` printed out. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way that doesn't require you create any additional files (but does require restarting your Julia session is the following):\n",
    "\n",
    "1. Add `Bloqade` and `BloqadeExpr` (see the instructions above, step 2. Note that while this procedure doesn't require a custom Julia environment, it is considered good practice to do so.)\n",
    "2. Exit the package management mode and type in `julia> using BloqadeExpr`\n",
    "3. Set your desired backend via `julia> BloqadeExpr.set_backend(\"backend_string_here\")` where \"backend_string_here\" can be any of the options listed in step 4 above.\n",
    "4. You will see a notice requesting you restart Julia. Exit the session.\n",
    "5. Upon restarting the session, launch Julia via `julia -t num_of_threads`\n",
    "    * As an additional sanity check, you can ensure Julia sees it has multiple threads available to it by importing `Base.Threads` and invoking the `nthreads()` function\n",
    "6. Type `julia> using BloqadeExpr` and then `BloqadeExpr.backend`.\n",
    "\n",
    "If you see the backend you provided earlier printed, Bloqade will now have multithreading enabled.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, re-run the `lattice_sweep_benchmark()` with multithreading enabled and see how much the performance improves. You should be able to achieve a ~3x speedup over the single threaded version!\n",
    "\n",
    "You can find some more information about multithreading in Bloqade via the documentation here: https://queracomputing.github.io/Bloqade.jl/dev/multithreading/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Support"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that needs to be done to run any simulations on a GPU is to move the `SchrodingerProblem` onto the GPU first, run the simulation, and then offload it! Even easier than multithreading, although there are some things that should be kept in mind:\n",
    "\n",
    "* The size of your problem is now limited to how much memory your GPU has\n",
    "* There is a one-time cost in terms of loading and off-loading your problem from GPU memory back into main memory\n",
    "  * But, even with this cost a great deal of performance benefit can be extracted!\n",
    "* Currently, Bloqade only supports single GPUs but multi-node/multi-GPU support is planned in the future"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the performance boost in action as well the procedure for using the GPU, we'll start by defining a problem for benchmarking (in this case, a 10-atom 1D chain with an adiabatic sweep applied to it)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, because of Julia's precompilation methods it's best to run this function twice so the precompilation overhead is not factored in as part of the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Bloqade\n",
    "\n",
    "function chain_run_time_CPU()\n",
    "\n",
    "    # define an adiabatic sweep to create a Z2 ordered phase on a chain of ten atoms\n",
    "\n",
    "    total_time = 3.0;\n",
    "    Ω_max = 2π * 4;\n",
    "    Ω = piecewise_linear(clocks = [0.0, 0.1, 2.1, 2.2, total_time], values = [0.0, Ω_max, Ω_max, 0, 0]);\n",
    "\n",
    "    U1 = -2π * 10;\n",
    "    U2 = 2π * 10;\n",
    "    Δ = piecewise_linear(clocks = [0.0, 0.6, 2.1, total_time], values = [U1, U1, U2, U2]);\n",
    "\n",
    "    nsites = 14\n",
    "    atoms = generate_sites(ChainLattice(), nsites, scale = 5.72)\n",
    "\n",
    "    h = rydberg_h(atoms; Δ, Ω)\n",
    "    reg = zero_state(nsites);\n",
    "    println(\"Creating SchrodingerProblem\")\n",
    "    problem = SchrodingerProblem(reg, total_time, h);\n",
    "\n",
    "    println(\"Measuring CPU time\")\n",
    "    @time emulate!(problem)\n",
    "    \n",
    "    # need to bring the problem back into main memory\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this on the GPU, you can take a look at the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Bloqade\n",
    "using Adapt\n",
    "using CUDA\n",
    "\n",
    "function chain_run_time_GPU()\n",
    "\n",
    "    # define an adiabatic sweep to create a Z2 ordered phase on a chain of fourteen atoms\n",
    "\n",
    "    total_time = 3.0;\n",
    "    Ω_max = 2π * 4;\n",
    "    Ω = piecewise_linear(clocks = [0.0, 0.1, 2.1, 2.2, total_time], values = [0.0, Ω_max, Ω_max, 0, 0]);\n",
    "\n",
    "    U1 = -2π * 10;\n",
    "    U2 = 2π * 10;\n",
    "    Δ = piecewise_linear(clocks = [0.0, 0.6, 2.1, total_time], values = [U1, U1, U2, U2]);\n",
    "\n",
    "    nsites = 14\n",
    "    atoms = generate_sites(ChainLattice(), nsites, scale = 5.72)\n",
    "\n",
    "    h = rydberg_h(atoms; Δ, Ω)\n",
    "    reg = zero_state(nsites);\n",
    "    println(\"Creating SchrodingerProblem\")\n",
    "    problem = SchrodingerProblem(reg, total_time, h; algo=DP8());\n",
    "\n",
    "    # Take advantage of multiple dispatch here, objects that are CuArrays \n",
    "    # automatically have GPU routines applied to them versus CPU implementation\n",
    "    println(\"Measuring GPU load/execution/unload time\")\n",
    "    @time begin\n",
    "        gpu_problem = adapt(CuArray, problem)\n",
    "        emulate!(gpu_problem)\n",
    "        offloaded_problem = adapt(Array, gpu_problem)\n",
    "    end\n",
    "    \n",
    "    # need to bring the problem back into main memory\n",
    "    return nothing\n",
    "\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice these lines:\n",
    "```julia\n",
    "gpu_problem = adapt(CuArray, problem)\n",
    "emulate!(gpu_problem)\n",
    "offloaded_problem = adapt(Array, gpu_problem)\n",
    "```\n",
    "\n",
    "We literally just take the `SchrodingerProblem`, `adapt` it onto the GPU, let the simulation run and then take it off for further analysis!\n",
    "\n",
    "In the interest of a comparable benchmark, note that the `@time` macro includes the loading and unloading times.\n",
    "\n",
    "You should find that there is still a healthy performance increase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now been exposed to Bloqade's High Performance capabilities.\n",
    "\n",
    "You've seen:\n",
    "* A birds-eye view of how Bloqade performs simulations\n",
    "* Choices for memory and execution efficiency\n",
    "* The different multithreading backends and their pros/cons\n",
    "* How to take advantage of Nvidia GPUs\n",
    "\n",
    "Now you can simulate many-body neutral atom dynamics *at scale* (: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
